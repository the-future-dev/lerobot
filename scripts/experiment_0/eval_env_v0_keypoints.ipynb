{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import imageio\n",
    "import gym_pusht  # noqa: F401\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from lerobot.scripts.eval import eval_policy\n",
    "from lerobot.common.policies.diffusion.modeling_diffusion import DiffusionPolicy\n",
    "from lerobot.common.policies.diffusion.configuration_diffusion import DiffusionConfig\n",
    "from lerobot.configs.types import PolicyFeature, FeatureType, NormalizationMode\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "\n",
    "output_directory = Path(\"../../outputs/eval/diffusion_pusht_keypoints\")\n",
    "output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "videos_dir = output_directory / \"videos\"\n",
    "videos_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:\n",
      "The dataset you requested (lerobot/pusht_keypoints) is in 2.0 format.\n",
      "While current version of LeRobot is backward-compatible with it, the version of your dataset still uses global\n",
      "stats instead of per-episode stats. Update your dataset stats to the new format using this command:\n",
      "```\n",
      "python lerobot/common/datasets/v21/convert_dataset_v20_to_v21.py --repo-id=lerobot/pusht_keypoints\n",
      "```\n",
      "\n",
      "If you encounter a problem, contact LeRobot maintainers on [Discord](https://discord.com/invite/s3KuuzsPFb)\n",
      "or open an [issue on GitHub](https://github.com/huggingface/lerobot/issues/new/choose).\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e12d5319bdb48c0a6299be29ee024fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = LeRobotDataset(\"lerobot/pusht_keypoints\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = Path(\"../../outputs/train/diffusion_pusht_keypoints\")\n",
    "# policy_config = DiffusionConfig.from_pretrained(pretrained_name_or_path=f\"{model_dir}/checkpoints/last/pretrained_model\", device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from local directory\n",
      "\n",
      "=== Policy Configuration ===\n",
      "Input features: {'observation.state': PolicyFeature(type=<FeatureType.STATE: 'STATE'>, shape=(2,)), 'observation.environment_state': PolicyFeature(type=<FeatureType.ENV: 'ENV'>, shape=(16,))}\n",
      "Output features: {'action': PolicyFeature(type=<FeatureType.ACTION: 'ACTION'>, shape=(2,))}\n",
      "Image features: {}\n",
      "Observation steps: 2\n",
      "Action steps: 8\n",
      "Horizon: 16\n"
     ]
    }
   ],
   "source": [
    "policy = DiffusionPolicy.from_pretrained(f\"{model_dir}/checkpoints/last/pretrained_model\")\n",
    "policy.to(device)\n",
    "\n",
    "# print(policy)\n",
    "print(\"\\n=== Policy Configuration ===\")\n",
    "print(f\"Input features: {policy.config.input_features}\")\n",
    "print(f\"Output features: {policy.config.output_features}\")\n",
    "print(f\"Image features: {policy.config.image_features}\")\n",
    "print(f\"Observation steps: {policy.config.n_obs_steps}\")\n",
    "print(f\"Action steps: {policy.config.n_action_steps}\")\n",
    "print(f\"Horizon: {policy.config.horizon}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x_andri/.conda/envs/lerobot/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment gym_pusht/PushT-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment observation space: Dict('agent_pos': Box(0.0, 512.0, (2,), float64), 'environment_state': Box(0.0, 512.0, (16,), float64), 'goal_state': Box(0.0, 512.0, (16,), float64))\n",
      "Environment action space: Box(0.0, 512.0, (2,), float32)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\n",
    "    \"gym_pusht/PushT-v0\",\n",
    "    obs_type=\"environment_state_agent_pos\",\n",
    "    max_episode_steps=300,\n",
    ")\n",
    "print(\"Environment observation space:\", env.observation_space)\n",
    "print(\"Environment action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pusht-v0 - keypoints env\n",
    "\n",
    "**Env outputs**: env.observation_space\n",
    "- \"environment_state\" => shape (16,)\n",
    "- \"goal_state\" => shape (16,)\n",
    "- \"agent_pos\" => shape (2,)\n",
    "\n",
    "**Env inputs**: env.action_space\n",
    "- shape (2,)\n",
    "\n",
    "### Diffusion Policy\n",
    "\n",
    "**Policy inputs**: policy.config.input_features\n",
    "- \"observation.environment_state\" => (32,)\n",
    "- \"observation.state\" => shape (2,)\n",
    "\n",
    "**Policy outputs**: policy.config.output_features\n",
    "- \"action\" => shape (2,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (680, 680) to (688, 688) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video of the evaluation is available in '../../outputs/eval/diffusion_pusht_keypoints/single_eval_rollout.mp4'.\n"
     ]
    }
   ],
   "source": [
    "# SINGLE ENVIRONMENT ROLLOUT #################################################################\n",
    "policy.reset()\n",
    "numpy_observation, info = env.reset(seed=10000)\n",
    "\n",
    "# Prepare to collect every rewards and all the frames of the episode,\n",
    "# from initial state to final state.\n",
    "rewards = []\n",
    "frames = []\n",
    "\n",
    "# Render frame of the initial state\n",
    "frames.append(env.render())\n",
    "\n",
    "step = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    state = torch.from_numpy(numpy_observation[\"agent_pos\"])\n",
    "    env_state = torch.from_numpy(numpy_observation[\"environment_state\"])\n",
    "    # env_state = torch.cat([env_state, \n",
    "    #                        torch.from_numpy(numpy_observation[\"goal_state\"])], dim=0)\n",
    "    \n",
    "    state = state.to(torch.float32).to(device, non_blocking=True)\n",
    "    env_state = env_state.to(torch.float32).to(device, non_blocking=True)\n",
    "    \n",
    "    # Batch dimension\n",
    "    state = state.unsqueeze(0)\n",
    "    env_state = env_state.unsqueeze(0)\n",
    "\n",
    "    policy_input = {\n",
    "        \"observation.state\": state,\n",
    "        \"observation.environment_state\": env_state\n",
    "    }\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        action = policy.select_action(policy_input)\n",
    "\n",
    "    numpy_action = action.squeeze(0).to(\"cpu\").numpy()\n",
    "\n",
    "    numpy_observation, reward, terminated, truncated, info = env.step(numpy_action)\n",
    "    # print(f\"{step=} {reward=} {terminated=}\")\n",
    "\n",
    "    rewards.append(reward)\n",
    "    frames.append(env.render())\n",
    "\n",
    "    done = terminated | truncated | done\n",
    "    step += 1\n",
    "\n",
    "if terminated:\n",
    "    print(\"Success!\")\n",
    "else:\n",
    "    print(\"Failure!\")\n",
    "\n",
    "# Get the speed of environment (i.e. its number of frames per second).\n",
    "fps = env.metadata[\"render_fps\"]\n",
    "\n",
    "# Encode all frames into a mp4 video.\n",
    "video_path = output_directory / \"single_eval_rollout.mp4\"\n",
    "imageio.mimsave(str(video_path), np.stack(frames), fps=fps)\n",
    "print(f\"Video of the evaluation is available in '{video_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCHED ROLLOUT for policy evaluation #####################################################################\n",
    "\n",
    "# Configure the number of environments and episodes\n",
    "n_envs = 3  # Number of parallel environments\n",
    "n_episodes = 3  # Total number of episodes to evaluate\n",
    "start_seed = 50  # Starting seed\n",
    "\n",
    "# Create a vectorized environment\n",
    "env_config = make_env_config(\n",
    "    env_type=\"pusht\",\n",
    "    obs_type=\"pixels_agent_pos\",\n",
    "    render_mode=\"rgb_array\",\n",
    "\n",
    ")\n",
    "# print(env_config)\n",
    "\n",
    "# Create the vectorized environment\n",
    "env = make_env(\n",
    "    env_config, \n",
    "    n_envs=n_envs,\n",
    "    use_async_envs=True  # Using AsyncVectorEnv for better performance\n",
    ")\n",
    "\n",
    "print(f\"Created vectorized environment with {n_envs} parallel environments\")\n",
    "print(f\"Running evaluation for {n_episodes} episodes starting from seed {start_seed}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = eval_policy(\n",
    "    env=env,\n",
    "    policy=policy,\n",
    "    n_episodes=n_episodes,\n",
    "    max_episodes_rendered=10,  # Only render 1 video\n",
    "    videos_dir=videos_dir,\n",
    "    return_episode_data=False,\n",
    "    start_seed=start_seed,\n",
    ")\n",
    "\n",
    "# Close the environment\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the aggregated metrics\n",
    "print(\"\\n=== Evaluation Results ===\")\n",
    "print(f\"Average sum reward: {eval_results['aggregated']['avg_sum_reward']:.4f}\")\n",
    "print(f\"Average max reward: {eval_results['aggregated']['avg_max_reward']:.4f}\")\n",
    "print(f\"Success rate: {eval_results['aggregated']['pc_success']:.2f}%\")\n",
    "print(f\"Total evaluation time: {eval_results['aggregated']['eval_s']:.2f} seconds\")\n",
    "print(f\"Average time per episode: {eval_results['aggregated']['eval_ep_s']:.2f} seconds\")\n",
    "\n",
    "# If you want to analyze per-episode results\n",
    "success_by_episode = [ep[\"success\"] for ep in eval_results[\"per_episode\"]]\n",
    "rewards_by_episode = [ep[\"sum_reward\"] for ep in eval_results[\"per_episode\"]]\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot rewards\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_by_episode)\n",
    "plt.title('Rewards by Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Sum Reward')\n",
    "\n",
    "# Plot success\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot([int(s) for s in success_by_episode])\n",
    "plt.title('Success by Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Success Rate')\n",
    "plt.ylim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_directory / \"evaluation_results.png\")\n",
    "plt.show()\n",
    "\n",
    "# Save the evaluation results to a file\n",
    "with open(output_directory / \"eval_results.json\", \"w\") as f:\n",
    "    json.dump(eval_results, f, indent=2)\n",
    "\n",
    "# Print video path if any videos were generated\n",
    "if \"video_paths\" in eval_results and eval_results[\"video_paths\"]:\n",
    "    print(f\"\\nGenerated video is available at: {eval_results['video_paths'][0]}\")\n",
    "else:\n",
    "    print(\"No videos generated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965281bdd77247179deeca886a1bff83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b6d9a6e6336407bb2033de3ea8608fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.05G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/the-future-dev/diffusion-pusht-keypoints/commit/2294bc4519685ceebe9ae97f8576e56d9d9db65f', commit_message='DiffusionPolicy for Pusht trained with keypoints of the current T position and the end T position', commit_description='', oid='2294bc4519685ceebe9ae97f8576e56d9d9db65f', pr_url=None, repo_url=RepoUrl('https://huggingface.co/the-future-dev/diffusion-pusht-keypoints', endpoint='https://huggingface.co', repo_type='model', repo_id='the-future-dev/diffusion-pusht-keypoints'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy.save_pretrained(\n",
    "    \"the-future-dev/diffusion-pusht-keypoints\",\n",
    "    push_to_hub=True,\n",
    "    commit_message=\"DiffusionPolicy for Pusht trained with keypoints of the current T position and the end T position\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del policy\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
