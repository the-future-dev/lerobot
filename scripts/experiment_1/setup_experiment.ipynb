{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To setup this experiment, we need to:\n",
    "\n",
    "1. Modify the current environment to include the endgoal keypoints.\n",
    "2. Modify the current policy to include the endgoal keypoints.\n",
    "3. We will use the same dataset as the baseline, but we will add the endgoal keypoints to the dataset. We will exploit the caviat that the endgoal keypoints do not change in the baseline dataset.\n",
    "4. Train a new policy on the collected data.\n",
    "5. Evaluate the new policy.\n",
    "\n",
    "NOTE the success performance is not expected to increase:\n",
    "1. the endgoal keypoints are constant the baseline dataset.\n",
    "2. the policy baseline already achieves good result.\n",
    "\n",
    "The experiment is relevant to evaluate if the training process is affected by the privileged information.\n",
    "-> What we could expect might be an increase in training stability or speed, as the policy will have access to the endgoal keypoints.\n",
    "\n",
    "Reasons the experiment might fail:\n",
    "1. Behaviour cloning is not based on understaning the task, but on imitating the expert policy.\n",
    "\n",
    "Reasons the experiment might succeed:\n",
    "1. The policy will have access to the endgoal keypoints, which might help to understand the task better.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import imageio\n",
    "import gym_pusht  # noqa: F401\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the eval_policy function from the lerobot scripts\n",
    "from lerobot.scripts.eval import eval_policy\n",
    "from lerobot.configs.train import TrainPipelineConfig\n",
    "from lerobot.configs.default import DatasetConfig, EvalConfig\n",
    "from lerobot.configs.types import PolicyFeature, FeatureType, NormalizationMode\n",
    "from lerobot.common.policies.diffusion.configuration_diffusion import DiffusionConfig\n",
    "from lerobot.common.policies.diffusion.modeling_diffusion import DiffusionPolicy\n",
    "from lerobot.common.envs.factory import make_env, make_env_config\n",
    "from lerobot.common.policies.factory import make_policy\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment observation space: Dict('agent_pos': Box(0.0, 512.0, (2,), float64), 'environment_state': Box(0.0, 512.0, (32,), float64))\n",
      "Environment action space: Box(0.0, 512.0, (2,), float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arito/miniconda3/envs/lerobot/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment gym_pusht/PushT-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "# Environment Configuration ################################################################\n",
    "env = gym.make(\n",
    "    \"gym_pusht/PushT-v0\",\n",
    "    obs_type=\"environment_state_agent_pos_privileged\",\n",
    "    max_episode_steps=300,\n",
    ")\n",
    "print(\"Environment observation space:\", env.observation_space)\n",
    "print(\"Environment action space:\", env.action_space)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation: dict_keys(['environment_state', 'agent_pos'])\n",
      "Info: dict_keys(['pos_agent', 'vel_agent', 'block_pose', 'goal_pose', 'n_contacts', 'is_success'])\n"
     ]
    }
   ],
   "source": [
    "observation, info = env.reset(seed=4120312)\n",
    "print(f\"Observation: {observation.keys()}\")\n",
    "print(f\"Info: {info.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "926237d18bf14a92a44ebe579cac46f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/206 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_config = DatasetConfig(\n",
    "    repo_id=\"lerobot/pusht_keypoints\"\n",
    ")\n",
    "\n",
    "dataset = LeRobotDataset(repo_id=\"the-future-dev/pusht_keypoints_expanded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATASET REFACTORING:\n",
    "<code>\n",
    "dataset_config = DatasetConfig(\n",
    "    repo_id=\"lerobot/pusht_keypoints\"\n",
    ")\n",
    "\n",
    "dataset = LeRobotDataset(repo_id=\"lerobot/pusht_keypoints\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from lerobot.common.datasets.lerobot_dataset import LeRobotDataset\n",
    "\n",
    "def create_privileged_dataset(original_dataset):\n",
    "    \"\"\"\n",
    "    Creates a new dataset with expanded environment_state feature (32,) by combining\n",
    "    environment_state and goal_state from the original dataset, preserving episode structure.\n",
    "    \"\"\"\n",
    "    # Create a copy of the features dictionary with modified environment_state shape\n",
    "    features = original_dataset.features.copy()\n",
    "    features[\"observation.environment_state\"] = {\n",
    "        \"dtype\": \"float32\",\n",
    "        \"shape\": (32,),  # Change from (16,) to (32,)\n",
    "        \"names\": features[\"observation.environment_state\"][\"names\"]  # Keep same names\n",
    "    }\n",
    "    \n",
    "    # Create a new dataset\n",
    "    new_repo_id = \"the-future-dev/pusht_keypoints_expanded\"\n",
    "    new_dataset = LeRobotDataset.create(\n",
    "        repo_id=new_repo_id,\n",
    "        fps=original_dataset.fps,\n",
    "        robot_type=original_dataset.meta.robot_type,\n",
    "        features=features\n",
    "    )\n",
    "    \n",
    "    # Get episode boundaries from the original dataset\n",
    "    episode_data_index = original_dataset.episode_data_index\n",
    "    episode_indices = list(range(len(episode_data_index[\"from\"])))  # List of episode indices (0 to num_episodes-1)\n",
    "    \n",
    "    print(f\"Copying and transforming data from {len(original_dataset)} frames across {len(episode_indices)} episodes...\")\n",
    "    \n",
    "    # Process each episode\n",
    "    for ep_idx in episode_indices:\n",
    "        if ep_idx % 10 == 0:  # Progress update every 10 episodes\n",
    "            print(f\"Processing episode {ep_idx}/{len(episode_indices)}\")\n",
    "        \n",
    "        # Get frame indices for this episode\n",
    "        from_idx = episode_data_index[\"from\"][ep_idx].item()\n",
    "        to_idx = episode_data_index[\"to\"][ep_idx].item()\n",
    "        \n",
    "        # Reset episode buffer for the new episode\n",
    "        new_dataset.episode_buffer = new_dataset.create_episode_buffer(episode_index=ep_idx)\n",
    "        \n",
    "        # Process all frames in this episode\n",
    "        for idx in range(from_idx, to_idx):\n",
    "            item = original_dataset[idx]\n",
    "            \n",
    "            # Create a copy of the frame, excluding unnecessary keys but keeping task\n",
    "            new_frame = {k: v for k, v in item.items() if k not in ['index', 'episode_index', 'frame_index', \"task_index\", \"timestamp\"]}\n",
    "            if \"task\" not in new_frame and \"task_index\" in new_frame:\n",
    "                # Map task_index back to task string if task is missing\n",
    "                task_idx = new_frame[\"task_index\"].item() if isinstance(new_frame[\"task_index\"], (torch.Tensor, np.ndarray)) else new_frame[\"task_index\"]\n",
    "                new_frame[\"task\"] = original_dataset.meta.tasks[task_idx]\n",
    "            \n",
    "            # Combine environment_state and goal_state\n",
    "            env_state = item[\"observation.environment_state\"]\n",
    "            goal_state = np.array([224.18019485, 266.60660172, 245.39339828, 287.81980515,\n",
    "                                   181.75378798, 351.45941546, 160.54058454, 330.24621202,\n",
    "                                   213.57359313, 213.57359313, 298.42640687, 298.42640687,\n",
    "                                   277.21320344, 319.63961031, 192.36038969, 234.78679656], dtype=np.float32)\n",
    "            \n",
    "            # Ensure env_state is a tensor with float32 dtype\n",
    "            if not isinstance(env_state, torch.Tensor):\n",
    "                env_state = torch.tensor(env_state, dtype=torch.float32)\n",
    "            elif env_state.dtype != torch.float32:\n",
    "                env_state = env_state.to(dtype=torch.float32)\n",
    "            combined_state = torch.cat([env_state, torch.tensor(goal_state, dtype=torch.float32)])\n",
    "            \n",
    "            # Update the frame with the combined state\n",
    "            new_frame[\"observation.environment_state\"] = combined_state.numpy().astype(np.float32)\n",
    "            \n",
    "            # Fix scalar features to have shape (1,) with correct dtype\n",
    "            scalar_features = [\"next.reward\", \"next.success\", \"next.done\"]\n",
    "            for key in scalar_features:\n",
    "                if key in new_frame:\n",
    "                    expected_dtype = features[key][\"dtype\"] if isinstance(features[key], dict) else str(features[key])\n",
    "                    if isinstance(new_frame[key], (np.ndarray, torch.Tensor)) and new_frame[key].shape == ():\n",
    "                        value = new_frame[key].item()\n",
    "                        if expected_dtype == \"bool\":\n",
    "                            new_frame[key] = np.array([bool(value)], dtype=bool)\n",
    "                        elif expected_dtype == \"float32\":\n",
    "                            new_frame[key] = np.array([value], dtype=np.float32)\n",
    "                    elif isinstance(new_frame[key], (int, float)):\n",
    "                        if expected_dtype == \"bool\":\n",
    "                            new_frame[key] = np.array([bool(new_frame[key])], dtype=bool)\n",
    "                        elif expected_dtype == \"float32\":\n",
    "                            new_frame[key] = np.array([new_frame[key]], dtype=np.float32)\n",
    "            \n",
    "            # Add the frame to the episode buffer\n",
    "            new_dataset.add_frame(new_frame)\n",
    "        \n",
    "        new_dataset.save_episode()\n",
    "\n",
    "    # Push to hub and reload the dataset\n",
    "    print(\"Pushing dataset to hub...\")\n",
    "    new_dataset.push_to_hub(new_repo_id)\n",
    "    print(\"Reloading dataset from hub...\")\n",
    "    new_dataset = LeRobotDataset(repo_id=new_repo_id)  # Load from hub after pushing\n",
    "    \n",
    "    print(f\"Created new dataset with {len(new_dataset)} frames across {new_dataset.num_episodes} episodes\")\n",
    "    return new_dataset\n",
    "\n",
    "# Run the function\n",
    "privileged_dataset = create_privileged_dataset(dataset)\n",
    "\n",
    "# Verify the transformation worked\n",
    "sample = privileged_dataset[0]\n",
    "print(\"Original environment state shape:\", dataset[0][\"observation.environment_state\"].shape)\n",
    "print(\"Privileged environment state shape:\", sample[\"observation.environment_state\"].shape)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = Path(\"../../outputs/eval/pusht_keypoints_privileged\")\n",
    "output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "videos_dir = output_directory / \"videos\"\n",
    "videos_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policy Configuration ################################################################\n",
    "policy_config = DiffusionConfig(\n",
    "    # I/O structure.\n",
    "    n_obs_steps=2,\n",
    "    horizon=16,\n",
    "    n_action_steps=8,\n",
    "    input_features={\n",
    "        \"observation.state\": PolicyFeature(type=FeatureType.STATE, shape=(2,)),\n",
    "        \"observation.environment_state\": PolicyFeature(type=FeatureType.ENV, shape=(32,))\n",
    "    },\n",
    "    output_features={\n",
    "        \"action\": PolicyFeature(type=FeatureType.ACTION, shape=(2,))\n",
    "    },\n",
    "    normalization_mapping={\n",
    "        \"STATE\": NormalizationMode.MIN_MAX,\n",
    "        \"ENV\": NormalizationMode.MIN_MAX,\n",
    "        \"ACTION\": NormalizationMode.MIN_MAX,\n",
    "        \"VISUAL\": NormalizationMode.IDENTITY,\n",
    "    },\n",
    "    \n",
    "    # Architecture.\n",
    "    # State encoder parameters\n",
    "    state_backbone=\"MLP\",\n",
    "    state_encoder_block_channels=[64, 128],\n",
    "    state_encoder_feature_dim=256,\n",
    "    state_encoder_use_layernorm=True,\n",
    "    # Unet. => Default\n",
    "    # Noise scheduler.\n",
    "    noise_scheduler_type=\"DDPM\",\n",
    "    beta_schedule=\"squaredcos_cap_v2\",\n",
    "    beta_start=0.0001,\n",
    "    beta_end=0.02,\n",
    "    num_train_timesteps=100,\n",
    "    prediction_type=\"epsilon\",\n",
    "\n",
    "    # Training hyperparameters\n",
    "    # optimizer_lr=1e-4,\n",
    "    # optimizer_betas=(0.95, 0.999),\n",
    "    # optimizer_eps=1e-8,\n",
    "    # optimizer_weight_decay=1e-6,\n",
    "    # scheduler_name=\"cosine\",\n",
    "    # scheduler_warmup_steps=500,\n",
    "    device=device,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'lerobot.common.policies.diffusion.modeling_diffusion.DiffusionPolicy'>\n"
     ]
    }
   ],
   "source": [
    "policy = make_policy(policy_config, ds_meta=dataset.meta)\n",
    "print(type(policy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure the initialized policy can play in the environment before starting training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failure!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:imageio_ffmpeg:IMAGEIO FFMPEG_WRITER WARNING: input image is not divisible by macro_block_size=16, resizing from (680, 680) to (688, 688) to ensure video compatibility with most codecs and players. To prevent resizing, make your input image divisible by the macro_block_size or set the macro_block_size to 1 (risking incompatibility).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video of the evaluation is available in '../../outputs/eval/pusht_keypoints_privileged/single_eval_rollout.mp4'.\n"
     ]
    }
   ],
   "source": [
    "# SINGLE ENVIRONMENT ROLLOUT #################################################################\n",
    "\n",
    "policy.reset()\n",
    "numpy_observation, info = env.reset(seed=42)\n",
    "\n",
    "# Prepare to collect every rewards and all the frames of the episode,\n",
    "# from initial state to final state.\n",
    "rewards = []\n",
    "frames = []\n",
    "\n",
    "# Render frame of the initial state\n",
    "frames.append(env.render())\n",
    "\n",
    "step = 0\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    state = torch.from_numpy(numpy_observation[\"agent_pos\"])\n",
    "    environment_state = torch.from_numpy(numpy_observation[\"environment_state\"])\n",
    "\n",
    "    state = state.to(torch.float32).to(device, non_blocking=True)\n",
    "    environment_state = environment_state.to(torch.float32).to(device, non_blocking=True)\n",
    "\n",
    "    # Batch dimension\n",
    "    state = state.unsqueeze(0)\n",
    "    environment_state = environment_state.unsqueeze(0)\n",
    "\n",
    "    policy_input = {\n",
    "        \"observation.state\": state,\n",
    "        \"observation.environment_state\": environment_state\n",
    "    }\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        action = policy.select_action(policy_input)\n",
    "\n",
    "    numpy_action = action.squeeze(0).to(\"cpu\").numpy()\n",
    "\n",
    "    numpy_observation, reward, terminated, truncated, info = env.step(numpy_action)\n",
    "    # print(f\"{step=} {reward=} {terminated=}\")\n",
    "\n",
    "    rewards.append(reward)\n",
    "    frames.append(env.render())\n",
    "\n",
    "    done = terminated | truncated | done\n",
    "    step += 1\n",
    "\n",
    "if terminated:\n",
    "    print(\"Success!\")\n",
    "else:\n",
    "    print(\"Failure!\")\n",
    "\n",
    "# Get the speed of environment (i.e. its number of frames per second).\n",
    "fps = env.metadata[\"render_fps\"]\n",
    "\n",
    "# Encode all frames into a mp4 video.\n",
    "video_path = output_directory / \"single_eval_rollout.mp4\"\n",
    "imageio.mimsave(str(video_path), np.stack(frames), fps=fps)\n",
    "\n",
    "print(f\"Video of the evaluation is available in '{video_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "del policy\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lerobot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
